# IAP: 얼굴 및 손-물체 인식을 활용한 지능형 학습 감시 시스템

## Introduction
COVID-19 이후 비대면 교육과 자율 학습 환경이 일상화되면서, 많은 학습자들이 집중력 저하를 겪고 있다.   

지도자의 직접적인 피드백이 어려운 온라인 환경에서는 산만함, 졸음, 스마트폰 사용 등 외부 요인에 쉽게 노출되고, 이러한 주의력 결핍은 학습 효과 저하로 이어지는 사례도 존재한다. 하지만 현재 대부분의 시스템은 수동으로 캠 화면을 확인 및 기록하거나 얼굴 인식 기반의 단순 시선 추적 기능에 의존하고 있어 실질적인 집중도 확인을 정량적으로 파악하기 어렵다는 한계를 지닌다. 
  
본 프로젝트는 이러한 문제를 해결하고자, 실시간 집중도 판단이 가능한 지능형 학습 감시 시스템을 제안한다. 본 시스템은 웹캠 영상만으로 얼굴과 손 및 물체를 탐지하고, 얼굴 자세와 손에 쥔 물체의 정보를 종합 분석하여 학습자의 집중 여부를 판단한다. 플랫폼으로서 NVIDIA Jetson Nano 보드를 사용하며 엣지 환경에서 동작 가능하도록 모델을 경량화하고 실시간성 확보를 위한 최적화 기법을 적용하였다. 

## Basis
2.1 객체 탐지 기반 얼굴 및 손 인식
객체 탐지 분야에서는 YOLO(You Only Look Once) 시리즈가 속도와 정확도 측면에서 우수하여 실시간 시스템에 널리 채택되고 있다. 특히 YOLOv7은 이전 버전인 YOLOv4 및 YOLOv5에 비해 정확도와 처리 효율성이 향상되어[1], Jetson Nano와 같은 저전력 Edge 디바이스에서도 TensorRT를 통해 실시간 추론이 가능하다. 기존 head pose 추정 시스템의 대표 사례인 Yakhyo의 시스템[2]은 SCRFD 기반 얼굴 탐지기를 활용한 뒤 Hopenet으로 얼굴의 자세를 추정하는 방식을 채택하고 있다. 그러나 이 방법은 얼굴 외 다른 객체(예: 손, 스마트폰 등)를 인식할 수 없어 행동 분석의 확장성에 한계를 지닌다.
본 프로젝트는 SCRFD 대신 YOLOv7을 사용하여 얼굴뿐 아니라 손, 스마트폰, 펜 등 여러 객체를 동시에 탐지하도록 구현하였다. 이는 입력 이미지를 여러 모델에 전달하는 기존 방식에서 벗어나 단일 모델을 통해 모든 객체를 한 번에 탐지함으로써 처리 구조를 단순화하고 연산 효율성을 크게 높였다.
2.2 얼굴 자세 추정 (Head Pose Estimation)
얼굴 자세 추정 분야에서 널리 사용되는 방법으로는 CNN 기반의 회귀 모델인 Hopenet, PnP 기반의 OpenCV+dlib 조합, 그리고 landmark 기반의 MediaPipe Face Mesh 모델이 있다. Hopenet은 ResNet-18을 활용하여 yaw, pitch, roll의 회전 각도를 직접 예측하며 높은 정확도를 보이지만 GPU가 탑재된 환경에서 주로 작동된다. 반면 OpenCV+dlib 기반의 방법은 가벼운 연산량으로 Edge 디바이스에서 활용 가능하지만 조명과 표정 변화에 민감한 단점이 있고, MediaPipe는 상대적으로 낮은 정밀도로 인해 세밀한 자세 분석에 어려움이 있다.
본 프로젝트는 MobileNet 기반의 경량 CNN 모델을 TensorRT로 최적화하여 Jetson Nano에서도 높은 실시간 성능을 제공한다. 또한 기존 방식이 고정값 또는 자동으로 결정되는 기준 포즈(reference pose)를 사용하는 반면, 본 시스템은 사용자로부터 직접 calibrate 명령을 입력받아, 일정 프레임 동안의 자세 평균값을 개인 맞춤형 기준 포즈로 설정하도록 구성하였다. 이를 통해 사용자 개인의 미세한 자세 편차를 고려하여 더욱 정밀하고 현실적인 집중도 평가가 가능하게 되었다.
